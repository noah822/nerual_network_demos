{"cells":[{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import torch\n","import numpy as np\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(2.)\n","tensor(1.)\n","tensor(1.)\n"]}],"source":["'''\n","    1. Basic autograd with variable\n","    y = w * x + b\n","    differentiate w.r.t parameters\n","'''\n","x = torch.tensor(1., requires_grad=True)\n","w = torch.tensor(2., requires_grad=True)\n","b = torch.tensor(2., requires_grad=True)\n","\n","y = w * x + b\n","y.backward()\n","\n","print(x.grad)\n","print(w.grad)\n","print(b.grad)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w:  Parameter containing:\n","tensor([[ 0.4522, -0.1739, -0.1692],\n","        [ 0.3306, -0.1562, -0.5751]], requires_grad=True)\n","b:  Parameter containing:\n","tensor([ 0.2279, -0.3755], requires_grad=True)\n"]}],"source":["'''\n","    2. Basic autograd with fully connected layer\n","'''\n","# Create two dimensional tensor\n","x = torch.randn(10, 3)\n","y = torch.randn(10, 2)\n","\n","linear = nn.Linear(3, 2)\n","print ('w: ', linear.weight)\n","print ('b: ', linear.bias)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["loss:  1.515024185180664\n","dL/dw:  tensor([[ 0.9705,  0.1195, -0.8792],\n","        [ 1.5142, -0.0796, -0.5738]])\n","dL/db:  tensor([-0.3236, -0.4115])\n","loss after 1 step optimization:  1.4692074060440063\n"]}],"source":["optimizer = torch.optim.SGD(linear.parameters(), lr = 0.01)\n","\n","# forward pass\n","pred = linear(x)\n","\n","# compute loss\n","criterion = nn.MSELoss()\n","loss = criterion(pred, y)\n","\n","print('loss: ', loss.item())\n","\n","# back propagate\n","loss.backward()\n","\n","# print out the gradients\n","print('dL/dw: ', linear.weight.grad)\n","print('dL/db: ', linear.bias.grad)\n","\n","# 1-step gradient descent\n","optimizer.step()\n","\n","# print the loss after 1-step gradient descent\n","pred = linear(x)\n","loss = criterion(pred, y)\n","print('loss after 1 step optimization: ', loss.item())"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1 2]\n"," [3 4]]\n","tensor([[1, 2],\n","        [3, 4]])\n","[[1 2]\n"," [3 4]]\n"]}],"source":["'''\n","    3. Loading data from numpy\n","'''\n","x = np.array([[1,2], [3,4]])\n","\n","# convert numpy array to torch tensor\n","y = torch.from_numpy(x)\n","\n","# convert torch tensor to numpy array\n","z = y.numpy()\n","\n","print(x)\n","print(y)\n","print(z)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.1 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":2}
